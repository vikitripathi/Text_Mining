#
######### Text mining (simple) example.
#

# First, import the text. Here we are using the policy which
# governs RSS consulting services.

policy.HTML.page <- readLines("http://policy.unt.edu/policy/3-5")
length(policy.HTML.page)

# Next, we need to isolate the actual text of the policy's HTML
# page. This can take some investigating -- using the head and
# tail functions or simply pasting the HTML page into a text editor.

policy.HTML.page[172:188]
id.1 <- 3 + which(policy.HTML.page == "                    TOTAL UNIVERSITY        </div>")
id.2 <- id.1 + 5
text.data <- policy.HTML.page[id.1:id.2]
rm(policy.HTML.page)

# Now we are left with a vector object which contains 6 lines of
# text (i.e. each paragraph has become a character string line
# of the vector).

text.data

# Next, we need to remove the HTML tags from each line of text. Generally
# multiple characters can be given in the 'pattern' argument within one
# implimentation of 'gsub' function; but here we are using two implimentations
# so that we specifically remove each of the HTML (paragraph) tags while
# leaving in place all other instances of the letter 'p'.

td.1 <- gsub(pattern = "<p>", "", x = text.data,
             ignore.case = TRUE, perl = FALSE, fixed = FALSE, useBytes = FALSE)
td.2 <- gsub(pattern = "</p>", "", x = td.1,
             ignore.case = TRUE, perl = FALSE, fixed = FALSE, useBytes = FALSE)
text.d <- td.2; rm(text.data, td.1, td.2)
text.d

# Next, load the package for text mining (tm).

library(tm)

# Now, we need to convert the vector into a 'corpus' of text.

txt <- VectorSource(text.d); rm(text.d)
txt.corpus <- Corpus(txt); rm(txt)
inspect(txt.corpus)

# Next, we make some adjustments to the text; making everything
# lower case, removing punctuation, removing numbers, and
# removing common English stopwords. The 'tm_map' function
# allows us to apply transformation functions to a corpus.

txt.corpus <- tm_map(txt.corpus, content_transformer(tolower))
txt.corpus <- tm_map(txt.corpus, removePunctuation)
txt.corpus <- tm_map(txt.corpus, removeNumbers)
txt.corpus <- tm_map(txt.corpus, removeWords, stopwords("english"))

# Next we perform stemming, which truncates words (e.g., "compute",
# "computes" and "computing" all become "comput").

library(SnowballC)
txt.corpus <- tm_map(txt.corpus, stemDocument)
detach("package:SnowballC")
inspect(txt.corpus)

# Next, we remove all the empty spaces generated by isolating the
# word stems in the previous step.

txt.corpus <- tm_map(txt.corpus, stripWhitespace)
inspect(txt.corpus)

# Now we can actually begin to analyze the text. First, we
# create something called a Term Document Matrix (TDM) which
# is a matrix of frequency counts for each word used. Below
# we only show the first 20 words and their frequencies in each
# document (i.e. for us, each 'document' is a paragraph in the
# original policy).

tdm <- TermDocumentMatrix(txt.corpus)
inspect(tdm[1:20,])

# Next, we can begin to explore the tdm to find which words
# were used most. Below we specify that we want terms / words
# which were used 8 or more times (in all documents / paragraphs).

findFreqTerms(x = tdm, lowfreq = 8, highfreq = Inf)

# Finding words which 'associate' together. Here, we are specifying
# the Term Document Matrix to use, the term we want to find associates
# for, and the lowest acceptable correlation limit with that term. This
# returns a vector of terms which are associated with 'comput' at
# 0.60 or more (correlation) -- and reports each association in
# decending order.

findAssocs(x = tdm, term = "comput", corlimit = 0.6)

# If desired, terms which occur very infrequently (i.e. sparse terms) can
# be removed; leaving only the 'common' terms. Below, the 'sparse' argument
# refers to the MAXIMUM sparse-ness allowed for a term to be in the
# returned matrix; in other words, the larger the percentage, the more
# terms will be retained (smaller the percentage, the fewer (but more
# common) terms will be retained.

tdm.common.60 <- removeSparseTerms(x = tdm, sparse = 0.60)
tdm.common.20 <- removeSparseTerms(x = tdm, sparse = 0.20)

tdm              # 161 terms
tdm.common.60    #  22 terms
tdm.common.20    #   5 terms

inspect(tdm.common.60)
inspect(tdm.common.20)


# End.